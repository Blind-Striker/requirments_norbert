{"cells":[{"cell_type":"markdown","metadata":{"id":"_4NzfRZGk-Y-"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tobhey/NoRBERT/blob/master/Code/Apply_Pretrained_Model/Pretrained_NoRBERT_func_concerns.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"IiDlHo-_CRac"},"source":["# Binary classification of functional requirements concerns with pretrained NoRBERT model"]},{"cell_type":"markdown","metadata":{"id":"18lQiv4UBSdt"},"source":["This notebook can be used to load a pretrained model, trained on the task of binary classification of functional requirements concerns (Function, Data, Behavior) on the developed function concerns dataset. It can be used to classify a given input."]},{"cell_type":"markdown","metadata":{"id":"fSq4Qamxdkz2"},"source":["Note: some cells are hidden and only the title is shown. To display the code, double-click the cell to switch the display mode."]},{"cell_type":"markdown","metadata":{"id":"d-1y5KaxYVmP"},"source":["## Init"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GQN5E-FoUqsv","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title Install dependencies {display-mode: \"form\"}\n","!pip install pytorch-transformers fastprogress"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oeArf6jVVMzl","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title Import dependencies {display-mode: \"form\"}\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","\n","from fastai import *\n","from fastai.text import *\n","from fastai.callbacks import *\n","\n","from pytorch_transformers import BertTokenizer, BertPreTrainedModel, BertModel, BertConfig\n","from pytorch_transformers import AdamW"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FgN8QQ8YYkpU","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title Define Config classes {display-mode: \"form\"}\n","class Config(dict):\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","        for k, v in kwargs.items():\n","            setattr(self, k, v)\n","    \n","    def set(self, key, val):\n","        self[key] = val\n","        setattr(self, key, val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tIyeBgjnYp7M","vscode":{"languageId":"python"}},"outputs":[],"source":["# adapt this to your needs!\n","config_data = Config(\n","    root_folder = '.', # where is the root folder? Keep it that way if you want to load from Google Drive\n","    model_path = '/models/', # where is the folder for the model(s); relative to the root\n","    model_name = 'NoRBERT_Task5_Function_e16_OverSampling.pkl', # what is the model name? NoRBERT_Task5_Behavior_e16_OverSampling.pkl, NoRBERT_Task5_Data_e16_OverSampling.pkl\n",")\n","\n","load_from_gdrive = True # True, if you want to use Google Drive; else, False\n","gdrive_root_folder = '/content/drive/My Drive/NoRBERT/Code/Apply_Pretrained_Model/' # Set this to the Google Drive path. Starts with '/content/drive/' and then usually 'My Drive/*' for the files in your Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wtzha3q7QjjU","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title Check, if and what kind of GPU is used {display-mode: \"form\"}\n","cuda_available = torch.cuda.is_available()\n","if cuda_available:\n","    curr_device = torch.cuda.current_device()\n","    print(torch.cuda.get_device_name(curr_device))\n","device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LysOxgPvW0am","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title Init loading from Google Drive, if set in config above {display-mode: \"form\"}\n","if load_from_gdrive:\n","    from google.colab import drive\n","    # Connect to drive to load the corpus from there\n","    drive.mount('/content/drive', force_remount=True)\n","    config_data.root_folder = gdrive_root_folder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TNRRj6jIJrp2","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title Load/Define NoRBERT classes {display-mode: \"form\"}\n","class FastAiBertTokenizer(BaseTokenizer):\n","    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n","    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=512, **kwargs):\n","        self._pretrained_tokenizer = tokenizer\n","        self.max_seq_len = max_seq_len\n","\n","    def __call__(self, *args, **kwargs):\n","        return self\n","\n","    def tokenizer(self, t:str):\n","        \"\"\"Limits the maximum sequence length. Prepend with [CLS] and append [SEP]\"\"\"\n","        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]\n","\n","## \n","\n","class BertTokenizeProcessor(TokenizeProcessor):\n","    \"\"\"Special Tokenizer, where we remove sos/eos tokens since we add that ourselves in the tokenizer.\"\"\"\n","    def __init__(self, tokenizer):\n","        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n","\n","class BertNumericalizeProcessor(NumericalizeProcessor):\n","    \"\"\"Use a custom vocabulary to match the original BERT model.\"\"\"\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n","\n","def get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n","    return [BertTokenizeProcessor(tokenizer=tokenizer),\n","            NumericalizeProcessor(vocab=vocab)]\n","\n","class BertDataBunch(TextDataBunch):\n","    @classmethod\n","    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n","              tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n","              label_cols:IntsOrStrs=0, **kwargs) -> DataBunch:\n","        \"Create a `TextDataBunch` from DataFrames.\"\n","        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n","        # use our custom processors while taking tokenizer and vocab as kwargs\n","        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n","        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n","        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n","                      TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n","        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n","        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n","        return src.databunch(**kwargs)\n","\n","##\n","\n","class BertTextClassifier(BertPreTrainedModel):\n","    def __init__(self, model_name, num_labels):\n","        config = BertConfig.from_pretrained(model_name)\n","        super(BertTextClassifier, self).__init__(config)\n","        self.num_labels = num_labels\n","        \n","        self.bert = BertModel.from_pretrained(model_name, config=config)\n","        \n","        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n","\n","        #self.apply(self.init_weights)\n","    \n","    def forward(self, tokens, labels=None, position_ids=None, token_type_ids=None, attention_mask=None, head_mask=None):\n","        outputs = self.bert(tokens, position_ids=position_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, head_mask=head_mask)\n","        \n","        pooled_output = outputs[1]\n","        # According to documentation of pytorch-transformers, pooled output might not be the best \n","        # and youâ€™re often better with averaging or pooling the sequence of hidden-states for the whole input sequence \n","        #hidden_states = outputs[0]\n","        #pooled_output = torch.mean(hidden_states, 1)\n","\n","        dropout_output = self.dropout(pooled_output)\n","        logits = self.classifier(dropout_output)\n","\n","        activation = nn.Softmax(dim=1)\n","        probs = activation(logits)   \n","\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OK17yQAGV_bM","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title Load classifier {display-mode: \"form\"}\n","classifier = load_learner(config_data.root_folder + config_data.model_path, config_data.model_name)"]},{"cell_type":"markdown","metadata":{"id":"9IxRhtyyYbht"},"source":["## Predict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CxsXG_HaXg1s","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title {display-mode: \"form\"}\n","def predict(classifier, text):\n","    prediction = classifier.predict(text)\n","    prediction_class = prediction[1]\n","    label = classifier.data.classes[prediction_class]\n","    return label"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"both","id":"UMMYic_pa5Sy","vscode":{"languageId":"python"}},"outputs":[],"source":["#Set the labels, the classifier will output (in correct order!)\n","labels1 = ['not_Function', 'Function']\n","labels2 = ['not_Data', 'Data']\n","labels3 = ['not_Behavior', 'Behavior']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztKNDBCVaGfK","vscode":{"languageId":"python"}},"outputs":[],"source":["reqs_path = '/content/drive/My Drive/NoRBERT/requirments.csv'\n","# Read the csv file\n","data = pd.read_csv(reqs_path,error_bad_lines=False,sep=';')\n","\n","# Print it out if you want\n","for i in range(len(data)):\n","    # Get the row\n","    row = data.iloc[i]\n","    req = row['RequirementText']\n","    pred_class = predict(classifier, req)\n","    print(labels1[pred_class]) \n","print('--------------------------------')\n","# Print it out if you want\n","for i in range(len(data)):\n","    # Get the row\n","    row = data.iloc[i]\n","    req = row['RequirementText']\n","    pred_class = predict(classifier, req)\n","    print(labels2[pred_class]) \n","print('--------------------------------')\n","# Print it out if you want\n","for i in range(len(data)):\n","    # Get the row\n","    row = data.iloc[i]\n","    req = row['RequirementText']\n","    pred_class = predict(classifier, req)\n","    print(labels3[pred_class]) \n","print('--------------------------------')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Pretrained_NoRBERT_func_concerns.ipynb","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
